{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HaVG16QjeXKC",
    "outputId": "e2e8f3f1-c698-4e26-8a0f-c7ec1a9e885d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting fastavro\n",
      "  Downloading fastavro-1.6.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.4 MB 4.2 MB/s \n",
      "\u001b[?25hInstalling collected packages: fastavro\n",
      "Successfully installed fastavro-1.6.1\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting python-snappy\n",
      "  Downloading python_snappy-0.6.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (56 kB)\n",
      "\u001b[K     |████████████████████████████████| 56 kB 2.1 MB/s \n",
      "\u001b[?25hInstalling collected packages: python-snappy\n",
      "Successfully installed python-snappy-0.6.1\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting fastparquet\n",
      "  Downloading fastparquet-0.8.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 4.3 MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.18 in /usr/local/lib/python3.7/dist-packages (from fastparquet) (1.21.6)\n",
      "Collecting cramjam>=2.3.0\n",
      "  Downloading cramjam-2.5.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 51.0 MB/s \n",
      "\u001b[?25hRequirement already satisfied: pandas>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from fastparquet) (1.3.5)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.7/dist-packages (from fastparquet) (2022.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.0->fastparquet) (2022.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.0->fastparquet) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.1.0->fastparquet) (1.15.0)\n",
      "Installing collected packages: cramjam, fastparquet\n",
      "Successfully installed cramjam-2.5.0 fastparquet-0.8.1\n"
     ]
    }
   ],
   "source": [
    "!pip install fastavro\n",
    "!pip install python-snappy\n",
    "!pip install fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "C1PVxZuNeSHN"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import ntpath\n",
    "import json\n",
    "from fastavro import reader as rd, json_writer\n",
    "import csv\n",
    "import sys\n",
    "import pprint\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "gcx2qhMN__mw"
   },
   "outputs": [],
   "source": [
    "#Function to get the filename from path for both Window/Linux system.\n",
    "def path_leaf(path):\n",
    "    head, tail = ntpath.split(path)\n",
    "    return tail or ntpath.basename(head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9IwV_GXkZqHq"
   },
   "outputs": [],
   "source": [
    "def conversion_json(path):\n",
    "  #Spliting filename with the extention for the output file\n",
    "  fname = path_leaf(path)\n",
    "  fname = fname.split('.')[0]\n",
    "  if path.endswith('.avro'):\n",
    "    with open(fname+\".json\", \"w\") as json_file:\n",
    "      with open(path, \"rb\") as avro_file:\n",
    "          avro_reader = rd(avro_file)\n",
    "          json_writer(json_file, avro_reader.writer_schema, avro_reader)\n",
    "    json_file.close() \n",
    "    avro_file.close()\n",
    "    #Reformatting your JSON file to contain an array\n",
    "    newJson = [json.loads(line) for line in open(fname+'.json','r')]\n",
    "    #The json file where the output must be stored \n",
    "    out_file = open(fname+\".json\", \"w\") \n",
    "    #Dump json data into the output file with indent = 6    \n",
    "    json.dump(newJson, out_file,indent=6) \n",
    "    out_file.close()\n",
    "  elif path.endswith('.parquet'):\n",
    "    # Reading PARQUET data as Dataframe with Pyarrow\n",
    "    dataFrame = pd.read_parquet(path, engine='fastparquet')\n",
    "    result = dataFrame.to_json(orient=\"records\")\n",
    "    parsed = json.loads(result)\n",
    "    # json.dumps(parsed, indent=4)\n",
    "    out_file = open(fname+\".json\", \"w\")  \n",
    "    json.dump(parsed, out_file,indent=6) \n",
    "    out_file.close()\n",
    "  else:\n",
    "    #Read csv file and add to data\n",
    "    data = {}\n",
    "    count = 0\n",
    "    with open(path) as csvFile:\n",
    "      csvReader = csv.DictReader(csvFile)\n",
    "      for rows in csvReader:\n",
    "        data[count] = rows\n",
    "        count += 1\n",
    "    csvFile.close()\n",
    "    #Create new json file and write data on it\n",
    "    with open(fname+'.json','w') as jsonFile:\n",
    "      jsonFile.write(json.dumps(data, indent=4))\n",
    "    jsonFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "eCn2OA_srePV"
   },
   "outputs": [],
   "source": [
    "def createArraySchema(array_val):\n",
    "\t'''\n",
    "\tArrays add an unfortuante amount of complexity to this. So we search for the type and the act accordingly.\n",
    "\tthe assumption is made that the array is all of the same type to avoid extra computation.\n",
    "\tTODO; find a way to avoid duplicate code for the if statements finding which type it is\n",
    "\t'''\n",
    "\t## get type of array_val\n",
    "\tarr_type = type(array_val)\n",
    "\n",
    "\t## check type\n",
    "\tif arr_type is dict:\n",
    "\t\treturn createSchema(array_val)\n",
    "\telif arr_type is int: \n",
    "\t\treturn \"Int\"\n",
    "\tif arr_type is float:\n",
    "\t\treturn \"Float\"\n",
    "\telif arr_type is bool:\n",
    "\t\treturn \"Boolean\"\n",
    "\telif arr_type is str or arr_type is bytes:\n",
    "\t\treturn \"String\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ngPO6RvArf1c"
   },
   "outputs": [],
   "source": [
    "def createSchema(df):\n",
    "\t'''\n",
    "\tLoop through keys and create a dctionary that will represent the schema for the document. Recursively call on dictiaonarys to go down object.\n",
    "\n",
    "\t'''\n",
    "\t\n",
    "\tschema = {}\n",
    "\ttemp = {}\n",
    "\tcount = 0\n",
    "\tcolum = df.columns\n",
    "\tfor col in colum:\n",
    "\t\tfor row in df[col]:\n",
    "\t\t\tif type(row) is int:\n",
    "\t\t\t\tif 'Int' in temp: \n",
    "\t\t\t\t\ttemp['Int'] +=1\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\ttemp['Int'] = 1\n",
    "\t\t\telif type(row) is float:\n",
    "\t\t\t\tif 'Float' in temp: \n",
    "\t\t\t\t\ttemp['Float'] +=1\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\ttemp['Float'] = 1\n",
    "\t\t\telif type(row) is bool:\n",
    "\t\t\t\tif 'Boolean' in temp: \n",
    "\t\t\t\t\ttemp['Boolean'] +=1\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\ttemp['Boolean'] = 1\n",
    "\t\t\telif type(row) is str or type(row) is bytes:\n",
    "\t\t\t\tif 'String' in temp: \n",
    "\t\t\t\t\ttemp['String'] +=1\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\ttemp['String'] = 1\n",
    "\t\t\telif type(row) is list:\n",
    "\t\t\t\tif 'List' in temp: \n",
    "\t\t\t\t\ttemp['List'] +=1\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\ttemp['List of Type :'+createArraySchema(row)] = 1\n",
    "\t\t\telif type(row) is dict:\n",
    "\t\t\t\tif 'Dictionary' in temp: \n",
    "\t\t\t\t\ttemp['Dictionary'] +=1\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\ttemp['Dictionary'] = 1\n",
    "\t\t\telse:\n",
    "\t\t\t\tif 'NULL' in temp: \n",
    "\t\t\t\t\ttemp['NULL'] +=1\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\ttemp['NULL'] = 1\n",
    "\t\tmax_val = max(temp.values())\n",
    "\t\tmax_key = max(temp, key=temp.get)\n",
    "\t\tschema[col] = max_key\n",
    "\t\ttemp = {}\n",
    "\treturn schema\n",
    "\n",
    "# \t## create object schema\n",
    "# \tschema = {}\n",
    "  \n",
    "# \t## loop through keys\n",
    "# \tfor key in doc:\n",
    "# \t\t## get key type\n",
    "# \t\tkey_type = type(doc[key])\n",
    "\n",
    "# \t\t## change key from unicode to string\n",
    "# \t\tkey = str(key)\n",
    "\n",
    "# \t\t## Check which type this is\n",
    "# \t\tif key_type is int:\n",
    "# \t\t\tschema[key] = \"Int\"\n",
    "# \t\tif key_type is float:\n",
    "# \t\t\tschema[key] = \"Float\"\n",
    "# \t\telif key_type is bool:\n",
    "# \t\t\tschema[key] = \"Boolean\"\n",
    "# \t\telif key_type is str or key_type is bytes:\n",
    "# \t\t\tif len(doc[key].strip()):\n",
    "# \t\t\t\tschema[key] = \"String\"\n",
    "# \t\telif key_type is list:\n",
    "# \t\t\t## create array and add to current schema\n",
    "# \t\t\tschema[key] = [createArraySchema(doc[key][0])]\n",
    "# \t\telif key_type is dict:\n",
    "# \t\t\t## create object and add to current schema\n",
    "# \t\t\tschema[key] = createSchema(doc[key])\n",
    "# \t\telse:\n",
    "# \t\t\tprint(\"unknown type:\", key_type)\n",
    "\n",
    "# \t## return fnished schema\n",
    "# \treturn schema\n",
    "\n",
    "\n",
    "# def getSchema(file_name):\n",
    "# \t'''\n",
    "# \tOpen file and pass the json document to createSchema\n",
    "# \t'''\n",
    "# \twith open(file_name) as data_file:    \n",
    "# \t\tdoc = json.loads(data_file.read()) \n",
    "# \t\tfor key in doc:\n",
    "# \t\t\t\treturn createSchema(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dlxYOlOTsPAQ",
    "outputId": "c5c2bbf0-ef95-4147-a436-ebe3ba407412"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter file name: /Users/veles/Applications/dev/schema-inference/data/userdata1.avro\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\tfile_name = \"\"\n",
    "\tschema = defaultdict(list)\n",
    "\tcount = 'schema'\n",
    "\t## test if file name given in command line\n",
    "\tif len(sys.argv) == 2:\n",
    "\t\t## grab from command line\n",
    "\t\tfile_name = sys.argv[1]\n",
    "\telse:\n",
    "\t\t## Get file from user\n",
    "\t\tfile_name = input(\"Please enter file name: \")\n",
    "\t\tfname = path_leaf(file_name)\n",
    "\t\tfname = fname.split('.')[0]\n",
    "\t\tif file_name.endswith('.json'):\n",
    "\t\t\tdf = pd.read_json(file_name)\n",
    "\t\t\tschem = createSchema(df) \n",
    "\t\t\t# with open(file_name) as data_file:    \n",
    "\t\t\t# \tdoc = json.loads(data_file.read()) \n",
    "\t\t\t# \tfor schema in doc:\n",
    "\t\t\t# \t\tsch = createSchema(schema)\n",
    "\t\t\t# \t\tschema[doc] = sch\n",
    "\t\t\t#Create new json file and write data on it\n",
    "\t\t\twith open('schema'+fname+'.json','w') as jsonFile:\n",
    "\t\t\t\tjsonFile.write(json.dumps(schem, indent=4))\n",
    "\t\t\tjsonFile.close()\n",
    "\t\telse:\n",
    "\t\t\t\tconversion_json(file_name)\n",
    "\t\t\t\tdf = pd.read_json(fname+'.json')\n",
    "\t\t\t\tschem = createSchema(df)\n",
    "\t\t\t\t# with open(fname+'.json') as data_file:\n",
    "\t\t\t\t# \tdoc = json.loads(data_file.read())\n",
    "\t\t\t\t# \tdf = pd.read_json (r'usrdata')\n",
    "\t\t\t\t# \tfor key in doc:\t\n",
    "\t\t\t\t# \t\tsch = createSchema(key)\n",
    "\t\t\t\t# \t\tschema[count] = sch\n",
    "\t\t\t\t# \t\tprint(sch)\n",
    "\t\t\t\t#Create new json file and write data on it\n",
    "\t\t\t\twith open('schema'+fname+'.json','w') as jsonFile:\n",
    "\t\t\t\t\tjsonFile.write(json.dumps(schem, indent=4))\n",
    "\t\t\t\tjsonFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Eh5DDGsKDX-j"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected object or value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_json\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muserDataParquet.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m schema \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      3\u001b[0m arr \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/opt/anaconda3/envs/schema-inference/lib/python3.9/site-packages/pandas/util/_decorators.py:199\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 199\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/schema-inference/lib/python3.9/site-packages/pandas/util/_decorators.py:299\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    295\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting with Pandas version \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mversion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m all arguments of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    296\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00marguments\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m will be keyword-only\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    297\u001b[0m     )\n\u001b[1;32m    298\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(msg, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39mstacklevel)\n\u001b[0;32m--> 299\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/schema-inference/lib/python3.9/site-packages/pandas/io/json/_json.py:563\u001b[0m, in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, lines, chunksize, compression, nrows, storage_options)\u001b[0m\n\u001b[1;32m    560\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m json_reader\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m json_reader:\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/schema-inference/lib/python3.9/site-packages/pandas/io/json/_json.py:694\u001b[0m, in \u001b[0;36mJsonReader.read\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    692\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_object_parser(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_lines(data_lines))\n\u001b[1;32m    693\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 694\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_object_parser\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    696\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[0;32m/opt/anaconda3/envs/schema-inference/lib/python3.9/site-packages/pandas/io/json/_json.py:716\u001b[0m, in \u001b[0;36mJsonReader._get_object_parser\u001b[0;34m(self, json)\u001b[0m\n\u001b[1;32m    714\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframe\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 716\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43mFrameParser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseries\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, \u001b[38;5;28mbool\u001b[39m):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/schema-inference/lib/python3.9/site-packages/pandas/io/json/_json.py:831\u001b[0m, in \u001b[0;36mParser.parse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_numpy()\n\u001b[1;32m    830\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 831\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_no_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    834\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/schema-inference/lib/python3.9/site-packages/pandas/io/json/_json.py:1079\u001b[0m, in \u001b[0;36mFrameParser._parse_no_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1075\u001b[0m orient \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morient\n\u001b[1;32m   1077\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m orient \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1078\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;241m=\u001b[39m DataFrame(\n\u001b[0;32m-> 1079\u001b[0m         \u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprecise_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecise_float\u001b[49m\u001b[43m)\u001b[49m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1080\u001b[0m     )\n\u001b[1;32m   1081\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m orient \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1082\u001b[0m     decoded \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1083\u001b[0m         \u001b[38;5;28mstr\u001b[39m(k): v\n\u001b[1;32m   1084\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m loads(json, precise_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecise_float)\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   1085\u001b[0m     }\n",
      "\u001b[0;31mValueError\u001b[0m: Expected object or value"
     ]
    }
   ],
   "source": [
    "df = pd.read_json (r'userDataParquet.json')\n",
    "schema = {}\n",
    "arr = {}\n",
    "count = 0\n",
    "colum = df.columns\n",
    "for col in colum:\n",
    "  for row in df[col]:\n",
    "    if type(row) is int:\n",
    "      if 'Int' in arr: \n",
    "        arr['Int'] +=1\n",
    "      else:\n",
    "        arr['Int'] = 1\n",
    "    elif type(row) is float:\n",
    "      if 'Float' in arr: \n",
    "        arr['Float'] +=1\n",
    "      else:\n",
    "        arr['Float'] = 1\n",
    "    elif type(row) is bool:\n",
    "      if 'Boolean' in arr: \n",
    "        arr['Boolean'] +=1\n",
    "      else:\n",
    "        arr['Boolean'] = 1\n",
    "    elif type(row) is str or type(row) is bytes:\n",
    "      if 'String' in arr: \n",
    "        arr['String'] +=1\n",
    "      else:\n",
    "        arr['String'] = 1\n",
    "    elif type(row) is list:\n",
    "      if 'List' in arr: \n",
    "        arr['List'] +=1\n",
    "      else:\n",
    "        arr['List of Type :'+createArraySchema(row)] = 1\n",
    "    elif type(row) is dict:\n",
    "      if 'Dictionary' in arr: \n",
    "        arr['Dictionary'] +=1\n",
    "      else:\n",
    "        arr['Dictionary'] = 1\n",
    "    else:\n",
    "      if 'NULL' in arr: \n",
    "        arr['NULL'] +=1\n",
    "      else:\n",
    "        arr['NULL'] = 1\n",
    "  max_val = max(arr.values())\n",
    "  max_key = max(arr, key=arr.get)\n",
    "  schema[col] = max_key\n",
    "  arr = {}     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XMLN8o3xluo1",
    "outputId": "add592b9-5161-45f4-9559-2ccd23996b82"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'registration_dttm': 'Int',\n",
       " 'id': 'Int',\n",
       " 'first_name': 'String',\n",
       " 'last_name': 'String',\n",
       " 'email': 'String',\n",
       " 'gender': 'String',\n",
       " 'ip_address': 'String',\n",
       " 'cc': 'String',\n",
       " 'country': 'String',\n",
       " 'birthdate': 'String',\n",
       " 'salary': 'Float',\n",
       " 'title': 'String',\n",
       " 'comments': 'String'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# create random data\n",
    "data = {\n",
    "    'name1': ([ 'John', 'Jane', 'Bob', 'Alice', 'Sam' ], 'string', False),\n",
    "    'favorite_number': ([np.random.randint(1, 100) if np.random.random() > 0.5 else None for _ in range(5)], 'Int64', True),\n",
    "    'favorite_color': ([np.random.choice(['red', 'blue', 'green', None]) for _ in range(5)], 'string', True)\n",
    "}\n",
    "\n",
    "# create dataframe\n",
    "df = pd.DataFrame({k: v[0] for k, v in data.items()})\n",
    "\n",
    "# set data types and nullability\n",
    "for col, (data, dtype, nullable) in data.items():\n",
    "    df[col] = pd.Series(data, dtype=dtype)\n",
    "    df[col] = df[col].astype(dtype)\n",
    "    if not nullable:\n",
    "        df[col] = df[col].astype(pd.api.types.CategoricalDtype(categories=df[col].unique()))\n",
    "\n",
    "\n",
    "# save to parquet file on local machine\n",
    "file_path = '/Users/veles/Applications/dev/schema-inf/data/example.parquet'\n",
    "df.to_parquet(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "vx1o1p37lvNz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema: {'name1': CategoricalDtype(categories=['John', 'Jane', 'Bob', 'Alice', 'Sam'], ordered=False), 'favorite_number': Int64Dtype(), 'favorite_color': StringDtype}\n",
      "Not Null: {'name1': True, 'favorite_number': False, 'favorite_color': False}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read Parquet file and perform schema inference\n",
    "file_path = '/Users/veles/Applications/dev/schema-inf/data/example.parquet'\n",
    "df = pd.read_parquet(file_path, engine='pyarrow')\n",
    "\n",
    "# generate schema and column info\n",
    "schema = df.dtypes.to_dict()\n",
    "not_null = df.notnull().all().to_dict()\n",
    "\n",
    "# print schema and column info\n",
    "print(\"Schema:\", schema)\n",
    "print(\"Not Null:\", not_null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Object of type CategoricalDtype is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m output_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/veles/Applications/dev/schema-inf/data/example-output.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(output_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 21\u001b[0m     \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# print confirmation message\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJSON file saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/schema-inference/lib/python3.9/json/__init__.py:179\u001b[0m, in \u001b[0;36mdump\u001b[0;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    173\u001b[0m     iterable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(skipkeys\u001b[38;5;241m=\u001b[39mskipkeys, ensure_ascii\u001b[38;5;241m=\u001b[39mensure_ascii,\n\u001b[1;32m    174\u001b[0m         check_circular\u001b[38;5;241m=\u001b[39mcheck_circular, allow_nan\u001b[38;5;241m=\u001b[39mallow_nan, indent\u001b[38;5;241m=\u001b[39mindent,\n\u001b[1;32m    175\u001b[0m         separators\u001b[38;5;241m=\u001b[39mseparators,\n\u001b[1;32m    176\u001b[0m         default\u001b[38;5;241m=\u001b[39mdefault, sort_keys\u001b[38;5;241m=\u001b[39msort_keys, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\u001b[38;5;241m.\u001b[39miterencode(obj)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# could accelerate with writelines in some versions of Python, at\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m# a debuggability cost\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m    180\u001b[0m     fp\u001b[38;5;241m.\u001b[39mwrite(chunk)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/schema-inference/lib/python3.9/json/encoder.py:431\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_list(o, _current_indent_level)\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(o, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m--> 431\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_dict(o, _current_indent_level)\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/schema-inference/lib/python3.9/json/encoder.py:405\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    404\u001b[0m             chunks \u001b[38;5;241m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[0;32m--> 405\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    407\u001b[0m     _current_indent_level \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/schema-inference/lib/python3.9/json/encoder.py:405\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    404\u001b[0m             chunks \u001b[38;5;241m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[0;32m--> 405\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    407\u001b[0m     _current_indent_level \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/schema-inference/lib/python3.9/json/encoder.py:438\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCircular reference detected\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    437\u001b[0m     markers[markerid] \u001b[38;5;241m=\u001b[39m o\n\u001b[0;32m--> 438\u001b[0m o \u001b[38;5;241m=\u001b[39m \u001b[43m_default\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m _iterencode(o, _current_indent_level)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/schema-inference/lib/python3.9/json/encoder.py:179\u001b[0m, in \u001b[0;36mJSONEncoder.default\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, o):\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;124;03m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;124;03m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m \n\u001b[1;32m    178\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObject of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    180\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis not JSON serializable\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type CategoricalDtype is not JSON serializable"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# read Parquet file and perform schema inference\n",
    "file_path = '/Users/veles/Applications/dev/schema-inf/data/example.parquet'\n",
    "df = pd.read_parquet(file_path, engine='pyarrow')\n",
    "\n",
    "# generate schema and column info\n",
    "schema = df.dtypes.to_dict()\n",
    "not_null = df.notnull().all().to_dict()\n",
    "\n",
    "# create dictionary with all information\n",
    "json_dict = {\n",
    "    'schema': schema,\n",
    "    'not_null': not_null\n",
    "}\n",
    "\n",
    "# write dictionary to JSON file\n",
    "output_file = '/Users/veles/Applications/dev/schema-inf/data/example-output.json'\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(json_dict, f, indent=4)\n",
    "\n",
    "# print confirmation message\n",
    "print(f\"JSON file saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON file saved to /Users/veles/Applications/dev/schema-inf/data/example-output.json\n",
      "{\n",
      "    \"schema\": {\n",
      "        \"name1\": \"category\",\n",
      "        \"favorite_number\": \"Int64\",\n",
      "        \"favorite_color\": \"string\"\n",
      "    },\n",
      "    \"not_null\": {\n",
      "        \"name1\": true,\n",
      "        \"favorite_number\": false,\n",
      "        \"favorite_color\": false\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# read Parquet file and perform schema inference\n",
    "file_path = '/Users/veles/Applications/dev/schema-inf/data/example.parquet'\n",
    "df = pd.read_parquet(file_path, engine='pyarrow')\n",
    "\n",
    "# generate schema and column info\n",
    "schema = {}\n",
    "for col, dtype in df.dtypes.items():\n",
    "    if dtype.name == 'category':\n",
    "        schema[col] = str(dtype)\n",
    "    else:\n",
    "        schema[col] = str(dtype)\n",
    "not_null = df.notnull().all().to_dict()\n",
    "\n",
    "# create dictionary with all information\n",
    "json_dict = {\n",
    "    'schema': schema,\n",
    "    'not_null': not_null\n",
    "}\n",
    "\n",
    "# write dictionary to JSON file\n",
    "output_file = '/Users/veles/Applications/dev/schema-inf/data/example-output.json'\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(json_dict, f, indent=4)\n",
    "\n",
    "# print confirmation message\n",
    "print(f\"JSON file saved to {output_file}\")\n",
    "\n",
    "# print JSON file content\n",
    "with open(output_file, 'r') as f:\n",
    "    json_data = json.load(f)\n",
    "    json_str = json.dumps(json_data, indent=4)\n",
    "    print(json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "schema-inference",
   "language": "python",
   "name": "schema-inference"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
