{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HaVG16QjeXKC",
    "outputId": "e2e8f3f1-c698-4e26-8a0f-c7ec1a9e885d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting fastavro\n",
      "  Downloading fastavro-1.6.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.4 MB 4.2 MB/s \n",
      "\u001b[?25hInstalling collected packages: fastavro\n",
      "Successfully installed fastavro-1.6.1\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting python-snappy\n",
      "  Downloading python_snappy-0.6.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (56 kB)\n",
      "\u001b[K     |████████████████████████████████| 56 kB 2.1 MB/s \n",
      "\u001b[?25hInstalling collected packages: python-snappy\n",
      "Successfully installed python-snappy-0.6.1\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting fastparquet\n",
      "  Downloading fastparquet-0.8.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 4.3 MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.18 in /usr/local/lib/python3.7/dist-packages (from fastparquet) (1.21.6)\n",
      "Collecting cramjam>=2.3.0\n",
      "  Downloading cramjam-2.5.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 51.0 MB/s \n",
      "\u001b[?25hRequirement already satisfied: pandas>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from fastparquet) (1.3.5)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.7/dist-packages (from fastparquet) (2022.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.0->fastparquet) (2022.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.0->fastparquet) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.1.0->fastparquet) (1.15.0)\n",
      "Installing collected packages: cramjam, fastparquet\n",
      "Successfully installed cramjam-2.5.0 fastparquet-0.8.1\n"
     ]
    }
   ],
   "source": [
    "!pip install fastavro\n",
    "!pip install python-snappy\n",
    "!pip install fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "C1PVxZuNeSHN"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import ntpath\n",
    "import json\n",
    "from fastavro import reader as rd, json_writer\n",
    "import csv\n",
    "import sys\n",
    "import pprint\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "gcx2qhMN__mw"
   },
   "outputs": [],
   "source": [
    "#Function to get the filename from path for both Window/Linux system.\n",
    "def path_leaf(path):\n",
    "    head, tail = ntpath.split(path)\n",
    "    return tail or ntpath.basename(head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9IwV_GXkZqHq"
   },
   "outputs": [],
   "source": [
    "def conversion_json(path):\n",
    "  #Spliting filename with the extention for the output file\n",
    "  fname = path_leaf(path)\n",
    "  fname = fname.split('.')[0]\n",
    "  if path.endswith('.avro'):\n",
    "    with open(fname+\".json\", \"w\") as json_file:\n",
    "      with open(path, \"rb\") as avro_file:\n",
    "          avro_reader = rd(avro_file)\n",
    "          json_writer(json_file, avro_reader.writer_schema, avro_reader)\n",
    "    json_file.close() \n",
    "    avro_file.close()\n",
    "    #Reformatting your JSON file to contain an array\n",
    "    newJson = [json.loads(line) for line in open(fname+'.json','r')]\n",
    "    #The json file where the output must be stored \n",
    "    out_file = open(fname+\".json\", \"w\") \n",
    "    #Dump json data into the output file with indent = 6    \n",
    "    json.dump(newJson, out_file,indent=6) \n",
    "    out_file.close()\n",
    "  elif path.endswith('.parquet'):\n",
    "    # Reading PARQUET data as Dataframe with Pyarrow\n",
    "    dataFrame = pd.read_parquet(path, engine='fastparquet')\n",
    "    result = dataFrame.to_json(orient=\"records\")\n",
    "    parsed = json.loads(result)\n",
    "    # json.dumps(parsed, indent=4)\n",
    "    out_file = open(fname+\".json\", \"w\")  \n",
    "    json.dump(parsed, out_file,indent=6) \n",
    "    out_file.close()\n",
    "  else:\n",
    "    #Read csv file and add to data\n",
    "    data = {}\n",
    "    count = 0\n",
    "    with open(path) as csvFile:\n",
    "      csvReader = csv.DictReader(csvFile)\n",
    "      for rows in csvReader:\n",
    "        data[count] = rows\n",
    "        count += 1\n",
    "    csvFile.close()\n",
    "    #Create new json file and write data on it\n",
    "    with open(fname+'.json','w') as jsonFile:\n",
    "      jsonFile.write(json.dumps(data, indent=4))\n",
    "    jsonFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "eCn2OA_srePV"
   },
   "outputs": [],
   "source": [
    "def createArraySchema(array_val):\n",
    "\t'''\n",
    "\tArrays add an unfortuante amount of complexity to this. So we search for the type and the act accordingly.\n",
    "\tthe assumption is made that the array is all of the same type to avoid extra computation.\n",
    "\tTODO; find a way to avoid duplicate code for the if statements finding which type it is\n",
    "\t'''\n",
    "\t## get type of array_val\n",
    "\tarr_type = type(array_val)\n",
    "\n",
    "\t## check type\n",
    "\tif arr_type is dict:\n",
    "\t\treturn createSchema(array_val)\n",
    "\telif arr_type is int: \n",
    "\t\treturn \"Int\"\n",
    "\tif arr_type is float:\n",
    "\t\treturn \"Float\"\n",
    "\telif arr_type is bool:\n",
    "\t\treturn \"Boolean\"\n",
    "\telif arr_type is str or arr_type is bytes:\n",
    "\t\treturn \"String\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ngPO6RvArf1c"
   },
   "outputs": [],
   "source": [
    "def createSchema(df):\n",
    "\t'''\n",
    "\tLoop through keys and create a dctionary that will represent the schema for the document. Recursively call on dictiaonarys to go down object.\n",
    "\n",
    "\t'''\n",
    "\t\n",
    "\tschema = {}\n",
    "\ttemp = {}\n",
    "\tcount = 0\n",
    "\tcolum = df.columns\n",
    "\tfor col in colum:\n",
    "\t\tfor row in df[col]:\n",
    "\t\t\tif type(row) is int:\n",
    "\t\t\t\tif 'Int' in temp: \n",
    "\t\t\t\t\ttemp['Int'] +=1\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\ttemp['Int'] = 1\n",
    "\t\t\telif type(row) is float:\n",
    "\t\t\t\tif 'Float' in temp: \n",
    "\t\t\t\t\ttemp['Float'] +=1\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\ttemp['Float'] = 1\n",
    "\t\t\telif type(row) is bool:\n",
    "\t\t\t\tif 'Boolean' in temp: \n",
    "\t\t\t\t\ttemp['Boolean'] +=1\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\ttemp['Boolean'] = 1\n",
    "\t\t\telif type(row) is str or type(row) is bytes:\n",
    "\t\t\t\tif 'String' in temp: \n",
    "\t\t\t\t\ttemp['String'] +=1\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\ttemp['String'] = 1\n",
    "\t\t\telif type(row) is list:\n",
    "\t\t\t\tif 'List' in temp: \n",
    "\t\t\t\t\ttemp['List'] +=1\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\ttemp['List of Type :'+createArraySchema(row)] = 1\n",
    "\t\t\telif type(row) is dict:\n",
    "\t\t\t\tif 'Dictionary' in temp: \n",
    "\t\t\t\t\ttemp['Dictionary'] +=1\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\ttemp['Dictionary'] = 1\n",
    "\t\t\telse:\n",
    "\t\t\t\tif 'NULL' in temp: \n",
    "\t\t\t\t\ttemp['NULL'] +=1\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\ttemp['NULL'] = 1\n",
    "\t\tmax_val = max(temp.values())\n",
    "\t\tmax_key = max(temp, key=temp.get)\n",
    "\t\tschema[col] = max_key\n",
    "\t\ttemp = {}\n",
    "\treturn schema\n",
    "\n",
    "# \t## create object schema\n",
    "# \tschema = {}\n",
    "  \n",
    "# \t## loop through keys\n",
    "# \tfor key in doc:\n",
    "# \t\t## get key type\n",
    "# \t\tkey_type = type(doc[key])\n",
    "\n",
    "# \t\t## change key from unicode to string\n",
    "# \t\tkey = str(key)\n",
    "\n",
    "# \t\t## Check which type this is\n",
    "# \t\tif key_type is int:\n",
    "# \t\t\tschema[key] = \"Int\"\n",
    "# \t\tif key_type is float:\n",
    "# \t\t\tschema[key] = \"Float\"\n",
    "# \t\telif key_type is bool:\n",
    "# \t\t\tschema[key] = \"Boolean\"\n",
    "# \t\telif key_type is str or key_type is bytes:\n",
    "# \t\t\tif len(doc[key].strip()):\n",
    "# \t\t\t\tschema[key] = \"String\"\n",
    "# \t\telif key_type is list:\n",
    "# \t\t\t## create array and add to current schema\n",
    "# \t\t\tschema[key] = [createArraySchema(doc[key][0])]\n",
    "# \t\telif key_type is dict:\n",
    "# \t\t\t## create object and add to current schema\n",
    "# \t\t\tschema[key] = createSchema(doc[key])\n",
    "# \t\telse:\n",
    "# \t\t\tprint(\"unknown type:\", key_type)\n",
    "\n",
    "# \t## return fnished schema\n",
    "# \treturn schema\n",
    "\n",
    "\n",
    "# def getSchema(file_name):\n",
    "# \t'''\n",
    "# \tOpen file and pass the json document to createSchema\n",
    "# \t'''\n",
    "# \twith open(file_name) as data_file:    \n",
    "# \t\tdoc = json.loads(data_file.read()) \n",
    "# \t\tfor key in doc:\n",
    "# \t\t\t\treturn createSchema(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dlxYOlOTsPAQ",
    "outputId": "c5c2bbf0-ef95-4147-a436-ebe3ba407412"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter file name: /Users/veles/Applications/dev/schema-inference/data/userdata1.avro\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\tfile_name = \"\"\n",
    "\tschema = defaultdict(list)\n",
    "\tcount = 'schema'\n",
    "\t## test if file name given in command line\n",
    "\tif len(sys.argv) == 2:\n",
    "\t\t## grab from command line\n",
    "\t\tfile_name = sys.argv[1]\n",
    "\telse:\n",
    "\t\t## Get file from user\n",
    "\t\tfile_name = input(\"Please enter file name: \")\n",
    "\t\tfname = path_leaf(file_name)\n",
    "\t\tfname = fname.split('.')[0]\n",
    "\t\tif file_name.endswith('.json'):\n",
    "\t\t\tdf = pd.read_json(file_name)\n",
    "\t\t\tschem = createSchema(df) \n",
    "\t\t\t# with open(file_name) as data_file:    \n",
    "\t\t\t# \tdoc = json.loads(data_file.read()) \n",
    "\t\t\t# \tfor schema in doc:\n",
    "\t\t\t# \t\tsch = createSchema(schema)\n",
    "\t\t\t# \t\tschema[doc] = sch\n",
    "\t\t\t#Create new json file and write data on it\n",
    "\t\t\twith open('schema'+fname+'.json','w') as jsonFile:\n",
    "\t\t\t\tjsonFile.write(json.dumps(schem, indent=4))\n",
    "\t\t\tjsonFile.close()\n",
    "\t\telse:\n",
    "\t\t\t\tconversion_json(file_name)\n",
    "\t\t\t\tdf = pd.read_json(fname+'.json')\n",
    "\t\t\t\tschem = createSchema(df)\n",
    "\t\t\t\t# with open(fname+'.json') as data_file:\n",
    "\t\t\t\t# \tdoc = json.loads(data_file.read())\n",
    "\t\t\t\t# \tdf = pd.read_json (r'usrdata')\n",
    "\t\t\t\t# \tfor key in doc:\t\n",
    "\t\t\t\t# \t\tsch = createSchema(key)\n",
    "\t\t\t\t# \t\tschema[count] = sch\n",
    "\t\t\t\t# \t\tprint(sch)\n",
    "\t\t\t\t#Create new json file and write data on it\n",
    "\t\t\t\twith open('schema'+fname+'.json','w') as jsonFile:\n",
    "\t\t\t\t\tjsonFile.write(json.dumps(schem, indent=4))\n",
    "\t\t\t\tjsonFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Eh5DDGsKDX-j"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected object or value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_json\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muserDataParquet.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m schema \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      3\u001b[0m arr \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/opt/anaconda3/envs/schema-inference/lib/python3.9/site-packages/pandas/util/_decorators.py:199\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 199\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/schema-inference/lib/python3.9/site-packages/pandas/util/_decorators.py:299\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    295\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting with Pandas version \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mversion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m all arguments of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    296\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00marguments\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m will be keyword-only\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    297\u001b[0m     )\n\u001b[1;32m    298\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(msg, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39mstacklevel)\n\u001b[0;32m--> 299\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/schema-inference/lib/python3.9/site-packages/pandas/io/json/_json.py:563\u001b[0m, in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, lines, chunksize, compression, nrows, storage_options)\u001b[0m\n\u001b[1;32m    560\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m json_reader\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m json_reader:\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/schema-inference/lib/python3.9/site-packages/pandas/io/json/_json.py:694\u001b[0m, in \u001b[0;36mJsonReader.read\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    692\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_object_parser(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_lines(data_lines))\n\u001b[1;32m    693\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 694\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_object_parser\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    696\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[0;32m/opt/anaconda3/envs/schema-inference/lib/python3.9/site-packages/pandas/io/json/_json.py:716\u001b[0m, in \u001b[0;36mJsonReader._get_object_parser\u001b[0;34m(self, json)\u001b[0m\n\u001b[1;32m    714\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframe\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 716\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43mFrameParser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseries\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, \u001b[38;5;28mbool\u001b[39m):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/schema-inference/lib/python3.9/site-packages/pandas/io/json/_json.py:831\u001b[0m, in \u001b[0;36mParser.parse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_numpy()\n\u001b[1;32m    830\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 831\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_no_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    834\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/schema-inference/lib/python3.9/site-packages/pandas/io/json/_json.py:1079\u001b[0m, in \u001b[0;36mFrameParser._parse_no_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1075\u001b[0m orient \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morient\n\u001b[1;32m   1077\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m orient \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1078\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;241m=\u001b[39m DataFrame(\n\u001b[0;32m-> 1079\u001b[0m         \u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprecise_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecise_float\u001b[49m\u001b[43m)\u001b[49m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1080\u001b[0m     )\n\u001b[1;32m   1081\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m orient \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1082\u001b[0m     decoded \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1083\u001b[0m         \u001b[38;5;28mstr\u001b[39m(k): v\n\u001b[1;32m   1084\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m loads(json, precise_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecise_float)\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   1085\u001b[0m     }\n",
      "\u001b[0;31mValueError\u001b[0m: Expected object or value"
     ]
    }
   ],
   "source": [
    "df = pd.read_json (r'userDataParquet.json')\n",
    "schema = {}\n",
    "arr = {}\n",
    "count = 0\n",
    "colum = df.columns\n",
    "for col in colum:\n",
    "  for row in df[col]:\n",
    "    if type(row) is int:\n",
    "      if 'Int' in arr: \n",
    "        arr['Int'] +=1\n",
    "      else:\n",
    "        arr['Int'] = 1\n",
    "    elif type(row) is float:\n",
    "      if 'Float' in arr: \n",
    "        arr['Float'] +=1\n",
    "      else:\n",
    "        arr['Float'] = 1\n",
    "    elif type(row) is bool:\n",
    "      if 'Boolean' in arr: \n",
    "        arr['Boolean'] +=1\n",
    "      else:\n",
    "        arr['Boolean'] = 1\n",
    "    elif type(row) is str or type(row) is bytes:\n",
    "      if 'String' in arr: \n",
    "        arr['String'] +=1\n",
    "      else:\n",
    "        arr['String'] = 1\n",
    "    elif type(row) is list:\n",
    "      if 'List' in arr: \n",
    "        arr['List'] +=1\n",
    "      else:\n",
    "        arr['List of Type :'+createArraySchema(row)] = 1\n",
    "    elif type(row) is dict:\n",
    "      if 'Dictionary' in arr: \n",
    "        arr['Dictionary'] +=1\n",
    "      else:\n",
    "        arr['Dictionary'] = 1\n",
    "    else:\n",
    "      if 'NULL' in arr: \n",
    "        arr['NULL'] +=1\n",
    "      else:\n",
    "        arr['NULL'] = 1\n",
    "  max_val = max(arr.values())\n",
    "  max_key = max(arr, key=arr.get)\n",
    "  schema[col] = max_key\n",
    "  arr = {}     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XMLN8o3xluo1",
    "outputId": "add592b9-5161-45f4-9559-2ccd23996b82"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'registration_dttm': 'Int',\n",
       " 'id': 'Int',\n",
       " 'first_name': 'String',\n",
       " 'last_name': 'String',\n",
       " 'email': 'String',\n",
       " 'gender': 'String',\n",
       " 'ip_address': 'String',\n",
       " 'cc': 'String',\n",
       " 'country': 'String',\n",
       " 'birthdate': 'String',\n",
       " 'salary': 'Float',\n",
       " 'title': 'String',\n",
       " 'comments': 'String'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vx1o1p37lvNz"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "schema-inference",
   "language": "python",
   "name": "schema-inference"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
